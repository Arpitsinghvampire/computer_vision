{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7745916,"sourceType":"datasetVersion","datasetId":4528049},{"sourceId":7920415,"sourceType":"datasetVersion","datasetId":4654339}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-04T18:52:13.528681Z","iopub.execute_input":"2024-05-04T18:52:13.529065Z","iopub.status.idle":"2024-05-04T18:52:16.371344Z","shell.execute_reply.started":"2024-05-04T18:52:13.529035Z","shell.execute_reply":"2024-05-04T18:52:16.370100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing libraries\nimport cv2\nimport matplotlib.pyplot as plt\nfor i in range(1,5):\n    image = cv2.imread(f\"../input/darkface/image/{i}.png\")\n\n    if image is None:\n        print(\"Error: Unable to read the image.\")\n    else:\n        # Display the image\n        image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n        plt.imshow(image)\n        plt.axis(\"off\")\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# File path\nfile_path = \"/kaggle/input/darkface/label/4710.txt\"\n\n# Open the file\nwith open(file_path, 'r') as file:\n    # Read the contents\n    contents = file.read()\n\n# Print the contents\nprint(contents)\n\n#this part contains the number of people in the frame and the bounding box coordinates\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T12:32:08.782780Z","iopub.execute_input":"2024-04-19T12:32:08.783438Z","iopub.status.idle":"2024-04-19T12:32:08.794837Z","shell.execute_reply.started":"2024-04-19T12:32:08.783403Z","shell.execute_reply":"2024-04-19T12:32:08.793403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input/enhanced-input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-04-19T12:32:12.552658Z","iopub.execute_input":"2024-04-19T12:32:12.553330Z","iopub.status.idle":"2024-04-19T12:32:12.570879Z","shell.execute_reply.started":"2024-04-19T12:32:12.553292Z","shell.execute_reply":"2024-04-19T12:32:12.569858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing libraries\nimport cv2\nimport matplotlib.pyplot as plt\nfor i in range(1,5):\n    image = cv2.imread(f\"../input/darkface/image/{i}.png\")\n\n    if image is None:\n        print(\"Error: Unable to read the image.\")\n    else:\n        # Display the image\n        image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n        plt.imshow(image)\n        plt.axis(\"off\")\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-19T12:32:30.495704Z","iopub.execute_input":"2024-04-19T12:32:30.496340Z","iopub.status.idle":"2024-04-19T12:32:31.894506Z","shell.execute_reply.started":"2024-04-19T12:32:30.496308Z","shell.execute_reply":"2024-04-19T12:32:31.893252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# File path\nfile_path = \"/kaggle/input/darkface/label/4710.txt\"\n\n# Open the file\nwith open(file_path, 'r') as file:\n    # Read the contents\n    contents = file.read()\n\n# Print the contents\nprint(contents)\n\n#this part contains the number of people in the frame and the bounding box coordinates\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T12:32:36.473749Z","iopub.execute_input":"2024-04-19T12:32:36.474508Z","iopub.status.idle":"2024-04-19T12:32:36.481419Z","shell.execute_reply.started":"2024-04-19T12:32:36.474472Z","shell.execute_reply":"2024-04-19T12:32:36.479945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input/enhanced-input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nprint(\"done\")","metadata":{"execution":{"iopub.status.busy":"2024-04-19T12:32:42.020062Z","iopub.execute_input":"2024-04-19T12:32:42.020498Z","iopub.status.idle":"2024-04-19T12:32:42.039929Z","shell.execute_reply.started":"2024-04-19T12:32:42.020466Z","shell.execute_reply":"2024-04-19T12:32:42.038580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\n\nfor i in range(1, 5):\n    filepath = f\"/kaggle/input/enhanced-input/{i}.png\"\n    print(\"Trying to read:\", filepath)\n    image = cv2.imread(filepath)\n    if image is None:\n        print(\"Failed to read:\", filepath)\n        continue\n    \n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    plt.imshow(image)\n    plt.axis(\"off\")\n    plt.show()\nprint(\"done\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T18:53:19.481789Z","iopub.execute_input":"2024-05-04T18:53:19.482861Z","iopub.status.idle":"2024-05-04T18:53:20.761919Z","shell.execute_reply.started":"2024-05-04T18:53:19.482824Z","shell.execute_reply":"2024-05-04T18:53:20.760875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from glob import glob\nimport transformers\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom PIL import Image\nimport numpy as np\nfrom huggingface_hub import from_pretrained_keras","metadata":{"execution":{"iopub.status.busy":"2024-05-04T18:54:02.707450Z","iopub.execute_input":"2024-05-04T18:54:02.707843Z","iopub.status.idle":"2024-05-04T18:54:02.713843Z","shell.execute_reply.started":"2024-05-04T18:54:02.707807Z","shell.execute_reply":"2024-05-04T18:54:02.712836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_low_light_images=sorted(glob(\"/kaggle/input/darkface/image/*.png\"))[:400]\nval_low_light_images=sorted(glob(\"/kaggle/input/darkface/image/*.png\"))[400:500]\ntest_low_light=sorted(glob(\"/kaggle/input/darkface/image/*.png\"))[600:700]\nIMAGE_SIZE=256\nBATCH_SIZE=10\n\ndef load_data(file_path):\n    try:\n        image = tf.io.read_file(file_path)\n        image = tf.image.decode_png(image, channels=3)\n        image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n        image = image / 255.0\n        return image\n    except Exception as e:\n        print(f\"Error loading image {file_path}: {e}\")\n        return None\n\n\n\ndef data_generator(low_light_images):\n    dataset = tf.data.Dataset.from_tensor_slices((low_light_images))\n    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.filter(lambda x: x is not None)  # Filter out None values\n    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n    return dataset\n\ntrain_dataset=data_generator(train_low_light_images)\nval_dataset=data_generator(val_low_light_images)\n\nprint(\"TRAIN DATASET\",train_dataset)\nprint(\"VAL DATASET\",val_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T18:54:14.183250Z","iopub.execute_input":"2024-05-04T18:54:14.183667Z","iopub.status.idle":"2024-05-04T18:54:14.418648Z","shell.execute_reply.started":"2024-05-04T18:54:14.183634Z","shell.execute_reply":"2024-05-04T18:54:14.417534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_dce_net():\n    input_img=keras.Input(shape=[None,None,3])\n    conv1=layers.Conv2D(32,(3,3),activation=\"relu\",padding=\"same\")(input_img)\n    conv2=layers.Conv2D(32,(3,3),activation=\"relu\",padding=\"same\")(conv1)\n    conv3=layers.Conv2D(32,(3,3),activation=\"relu\",padding=\"same\")(conv2)\n    conv4=layers.Conv2D(32,(3,3),activation=\"relu\",padding=\"same\")(conv3)\n    \n    int_con1=layers.Concatenate(axis=-1)([conv4,conv3])\n    \n    conv5=layers.Conv2D(32,(3,3),activation=\"relu\",padding=\"same\")(int_con1)\n    \n    int_con2=layers.Concatenate(axis=-1)([conv5,conv2])\n    \n    conv6=layers.Conv2D(32,(3,3),activation=\"relu\",padding=\"same\")(int_con2)\n    \n    int_con3=layers.Concatenate(axis=-1)([conv6,conv1])\n    \n    x_r=layers.Conv2D(24,(3,3),activation=\"tanh\",padding=\"same\")(int_con3)\n    \n    return keras.Model(inputs=input_img,outputs=x_r)\nprint(\"done\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T12:56:13.902834Z","iopub.execute_input":"2024-04-19T12:56:13.903806Z","iopub.status.idle":"2024-04-19T12:56:13.917345Z","shell.execute_reply.started":"2024-04-19T12:56:13.903744Z","shell.execute_reply":"2024-04-19T12:56:13.915865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now here we define the loss functions for the dce net\n#the color constancy loss is used to correct the potential color devaitions  in the enhanced image\n\ndef color_constancy_loss(x):\n    mean_rgb=tf.reduce_mean(x,axis=(1,2),keepdims=True)\n    mr,mg,mb=mean_rgb[:,:,:,0],mean_rgb[:,:,:,1],mean_rgb[:,:,:,2]\n    d_rg=tf.square(mr-mg)\n    d_rb=tf.square(mr-mb)\n    d_gb=tf.square(mb-mg)\n    \n    return tf.sqrt(tf.square(d_rg)+tf.square(d_rb)+tf.square(d_gb))\nprint(\"done\")","metadata":{"execution":{"iopub.status.busy":"2024-04-19T13:03:11.236828Z","iopub.execute_input":"2024-04-19T13:03:11.237348Z","iopub.status.idle":"2024-04-19T13:03:11.246296Z","shell.execute_reply.started":"2024-04-19T13:03:11.237311Z","shell.execute_reply":"2024-04-19T13:03:11.245191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#exposure loss ,it measures  the distance between the average intensity value of a local\n#region and a preset well exposedness levels(set to 0.6)\n\ndef exposure_loss(x,mean_val=0.6):\n    x=tf.reduce_mean(x,axis=3,keepdims=True)\n    mean=tf.nn.avg_pool2d(x,ksize=16,strides=16,padding=\"VALID\")\n    return tf.reduce_mean(tf.square(mean-mean_val))\n\nprint(\"done\")","metadata":{"execution":{"iopub.status.busy":"2024-04-19T13:03:14.208135Z","iopub.execute_input":"2024-04-19T13:03:14.208551Z","iopub.status.idle":"2024-04-19T13:03:14.215941Z","shell.execute_reply.started":"2024-04-19T13:03:14.208517Z","shell.execute_reply":"2024-04-19T13:03:14.214717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#illumination smoothness loss\n#to preserve the monotonicity  relations betweent he neighbouring  pixels , the illumination  smoothness loss  is  added to each curve parmaeter map\ndef illumination_smoothness_loss(x):\n    batch_size=tf.shape(x)[0]\n    h_x=tf.shape(x)[1]\n    w_x=tf.shape(x)[2]\n    count_h = (tf.shape(x) [2] - 1) * tf.shape(x) [3]\n    count_w = tf.shape (x) [2] * (tf.shape (x) [3] - 1)\n    h_tv=tf.reduce_sum(tf.square((x[:,1:,:,:]-x[:,:h_x-1,:,:])))\n    w_tv=tf.reduce_sum(tf.square((x[:,:,1:,:]-x[:,:,:w_x-1,:])))\n    batch_size=tf.cast(batch_size,dtype=tf.float32)\n    count_h=tf.cast(count_h,dtype=tf.float32)\n    count_w=tf.cast(count_w,dtype=tf.float32)\n    return 2*(h_tv/count_h + w_tv/count_w)/batch_size\n\nprint(\"done\")\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-19T13:03:17.019366Z","iopub.execute_input":"2024-04-19T13:03:17.019792Z","iopub.status.idle":"2024-04-19T13:03:17.030972Z","shell.execute_reply.started":"2024-04-19T13:03:17.019759Z","shell.execute_reply":"2024-04-19T13:03:17.029661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\n\nclass SpatialConsistencyLoss(keras.losses.Loss):\n    def __init__(self, **kwargs):\n        super(SpatialConsistencyLoss, self).__init__(reduction='none')\n        self.left_kernel = tf.constant([[[[0,0,0]],[[-1,1,0]],[[0,0,0]]]], dtype=tf.float32)\n        self.right_kernel = tf.constant([[[[0,0,0]],[[0,1,-1]],[[0,0,0]]]], dtype=tf.float32)\n        self.up_kernel = tf.constant([[[[0,-1,0]],[[0,1,0]],[[0,0,0]]]], dtype=tf.float32)\n        self.down_kernel = tf.constant([[[[0,0,0]],[[0,1,0]],[[0,-1,0]]]], dtype=tf.float32)\n        \n    def call(self, y_true, y_pred):\n        original_mean = tf.reduce_mean(y_true, 3, keepdims=True)\n        enhanced_mean = tf.reduce_mean(y_pred, 3, keepdims=True)\n        \n        original_pool = tf.nn.avg_pool2d(original_mean, ksize=4, strides=4, padding=\"VALID\")\n        enhanced_pool = tf.nn.avg_pool2d(enhanced_mean, ksize=4, strides=4, padding='VALID')\n        \n        d_original_left = tf.nn.conv2d(original_pool, self.left_kernel, strides=[1,1,1,1], padding=\"SAME\")\n        d_original_right = tf.nn.conv2d(original_pool, self.right_kernel, strides=[1,1,1,1], padding=\"SAME\")\n        d_original_up = tf.nn.conv2d(original_pool, self.up_kernel, strides=[1,1,1,1], padding=\"SAME\")\n        d_original_down = tf.nn.conv2d(original_pool, self.down_kernel, strides=[1,1,1,1], padding=\"SAME\")\n        \n        d_enhanced_left = tf.nn.conv2d(enhanced_pool, self.left_kernel, strides=[1,1,1,1], padding=\"SAME\")\n        d_enhanced_right = tf.nn.conv2d(enhanced_pool, self.right_kernel, strides=[1,1,1,1], padding=\"SAME\")\n        d_enhanced_up = tf.nn.conv2d(enhanced_pool, self.up_kernel, strides=[1,1,1,1], padding=\"SAME\")\n        d_enhanced_down = tf.nn.conv2d(enhanced_pool, self.down_kernel, strides=[1,1,1,1], padding=\"SAME\")\n                                     \n        d_left = tf.square(d_original_left - d_enhanced_left)\n        d_right = tf.square(d_original_right - d_enhanced_right)\n        d_up = tf.square(d_original_up - d_enhanced_up)\n        d_down = tf.square(d_original_down - d_enhanced_down)\n                                     \n        return d_left + d_right + d_up + d_down\nprint(\"done\")","metadata":{"execution":{"iopub.status.busy":"2024-04-19T13:03:21.996016Z","iopub.execute_input":"2024-04-19T13:03:21.996474Z","iopub.status.idle":"2024-04-19T13:03:22.016000Z","shell.execute_reply.started":"2024-04-19T13:03:21.996438Z","shell.execute_reply":"2024-04-19T13:03:22.015019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ZeroDCE(keras.Model):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.dce_model = build_dce_net()\n\n    def compile(self, learning_rate, **kwargs):\n        super().compile(**kwargs)\n        self.optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n        self.spatial_constancy_loss = SpatialConsistencyLoss(reduction=\"none\")\n        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n        self.illumination_smoothness_loss_tracker = keras.metrics.Mean(\n            name=\"illumination_smoothness_loss\"\n        )\n        self.spatial_constancy_loss_tracker = keras.metrics.Mean(\n            name=\"spatial_constancy_loss\"\n        )\n        self.color_constancy_loss_tracker = keras.metrics.Mean(\n            name=\"color_constancy_loss\"\n        )\n        self.exposure_loss_tracker = keras.metrics.Mean(name=\"exposure_loss\")\n\n    @property\n    def metrics(self):\n        return [\n            self.total_loss_tracker,\n            self.illumination_smoothness_loss_tracker,\n            self.spatial_constancy_loss_tracker,\n            self.color_constancy_loss_tracker,\n            self.exposure_loss_tracker,\n        ]\n\n    def get_enhanced_image(self, data, output):\n        r1 = output[:, :, :, :3]\n        r2 = output[:, :, :, 3:6]\n        r3 = output[:, :, :, 6:9]\n        r4 = output[:, :, :, 9:12]\n        r5 = output[:, :, :, 12:15]\n        r6 = output[:, :, :, 15:18]\n        r7 = output[:, :, :, 18:21]\n        r8 = output[:, :, :, 21:24]\n        x = data + r1 * (tf.square(data) - data)\n        x = x + r2 * (tf.square(x) - x)\n        x = x + r3 * (tf.square(x) - x)\n        enhanced_image = x + r4 * (tf.square(x) - x)\n        x = enhanced_image + r5 * (tf.square(enhanced_image) - enhanced_image)\n        x = x + r6 * (tf.square(x) - x)\n        x = x + r7 * (tf.square(x) - x)\n        enhanced_image = x + r8 * (tf.square(x) - x)\n        return enhanced_image\n\n    def call(self, data):\n        dce_net_output = self.dce_model(data)\n        return self.get_enhanced_image(data, dce_net_output)\n\n    def compute_losses(self, data, output):\n        enhanced_image = self.get_enhanced_image(data, output)\n        loss_illumination = 200 * illumination_smoothness_loss(output)\n        loss_spatial_constancy = tf.reduce_mean(\n            self.spatial_constancy_loss(enhanced_image, data)\n        )\n        loss_color_constancy = 5 * tf.reduce_mean(color_constancy_loss(enhanced_image))\n        loss_exposure = 10 * tf.reduce_mean(exposure_loss(enhanced_image))\n        total_loss = (\n            loss_illumination\n            + loss_spatial_constancy\n            + loss_color_constancy\n            + loss_exposure\n        )\n\n        return {\n            \"total_loss\": total_loss,\n            \"illumination_smoothness_loss\": loss_illumination,\n            \"spatial_constancy_loss\": loss_spatial_constancy,\n            \"color_constancy_loss\": loss_color_constancy,\n            \"exposure_loss\": loss_exposure,\n        }\n\n    def train_step(self, data):\n        with tf.GradientTape() as tape:\n            output = self.dce_model(data)\n            losses = self.compute_losses(data, output)\n\n        gradients = tape.gradient(\n            losses[\"total_loss\"], self.dce_model.trainable_weights\n        )\n        self.optimizer.apply_gradients(zip(gradients, self.dce_model.trainable_weights))\n\n        self.total_loss_tracker.update_state(losses[\"total_loss\"])\n        self.illumination_smoothness_loss_tracker.update_state(\n            losses[\"illumination_smoothness_loss\"]\n        )\n        self.spatial_constancy_loss_tracker.update_state(\n            losses[\"spatial_constancy_loss\"]\n        )\n        self.color_constancy_loss_tracker.update_state(losses[\"color_constancy_loss\"])\n        self.exposure_loss_tracker.update_state(losses[\"exposure_loss\"])\n\n        return {metric.name: metric.result() for metric in self.metrics}\n\n    def test_step(self, data):\n        output = self.dce_model(data)\n        losses = self.compute_losses(data, output)\n\n        self.total_loss_tracker.update_state(losses[\"total_loss\"])\n        self.illumination_smoothness_loss_tracker.update_state(\n            losses[\"illumination_smoothness_loss\"]\n        )\n        self.spatial_constancy_loss_tracker.update_state(\n            losses[\"spatial_constancy_loss\"]\n        )\n        self.color_constancy_loss_tracker.update_state(losses[\"color_constancy_loss\"])\n        self.exposure_loss_tracker.update_state(losses[\"exposure_loss\"])\n\n        return {metric.name: metric.result() for metric in self.metrics}\n\n    def save_weights(self, filepath, overwrite=True, save_format=None, options=None):\n        \"\"\"While saving the weights, we simply save the weights of the DCE-Net\"\"\"\n        self.dce_model.save_weights(\n            filepath,\n            overwrite=overwrite,\n            save_format=save_format,\n            options=options,\n        )\n\n    def load_weights(self, filepath, by_name=False, skip_mismatch=False, options=None):\n        \"\"\"While loading the weights, we simply load the weights of the DCE-Net\"\"\"\n        self.dce_model.load_weights(\n            filepath=filepath,\n            by_name=by_name,\n            skip_mismatch=skip_mismatch,\n            options=options,\n        )\n\n        \nprint('done')","metadata":{"execution":{"iopub.status.busy":"2024-04-19T13:03:25.319472Z","iopub.execute_input":"2024-04-19T13:03:25.319879Z","iopub.status.idle":"2024-04-19T13:03:25.351586Z","shell.execute_reply.started":"2024-04-19T13:03:25.319847Z","shell.execute_reply":"2024-04-19T13:03:25.350317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now  we need to train our model\nzero_dce_model=ZeroDCE()\nzero_dce_model.compile(learning_rate=1e-4)\nhistory=zero_dce_model.fit(train_dataset,validation_data=val_dataset,epochs=4)\n\nprint(\"done\")","metadata":{"execution":{"iopub.status.busy":"2024-04-19T13:03:32.927624Z","iopub.execute_input":"2024-04-19T13:03:32.928287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(history.history)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:16:50.561940Z","iopub.execute_input":"2024-04-18T18:16:50.562365Z","iopub.status.idle":"2024-04-18T18:16:50.568470Z","shell.execute_reply.started":"2024-04-18T18:16:50.562333Z","shell.execute_reply":"2024-04-18T18:16:50.567502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_result(item):\n    # Check if the item exists in history\n    if item in history.history:\n        plt.plot(history.history[item], label=item)\n        # Check if the corresponding validation item exists in history\n        if \"val_\" + item in history.history:\n            plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n        plt.xlabel(\"Epochs\")\n        plt.ylabel(item)\n        plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n        plt.legend()\n        plt.grid()\n        plt.show()\n    else:\n        print(f\"'{item}' not found in history.\")\n\n    \nplot_result(\"total_loss\")\nplot_result(\"illumination_smootheness_loss\")\nplot_result(\"spatial_consistency_loss\")\nplot_result(\"color_constancy_loss\")\nplot_result(\"exposure_loss\")","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:19:36.052808Z","iopub.execute_input":"2024-04-18T18:19:36.053302Z","iopub.status.idle":"2024-04-18T18:19:36.968787Z","shell.execute_reply.started":"2024-04-18T18:19:36.053273Z","shell.execute_reply":"2024-04-18T18:19:36.967685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def infer(original_image):\n    image = keras.utils.img_to_array(original_image)\n    image = image.astype(\"float32\") / 255.0\n    image = np.expand_dims(image, axis=0)\n    output_image = zero_dce_model(image)\n    output_image = tf.cast((output_image[0, :, :, :] * 255), dtype=np.uint8)\n    output_image = Image.fromarray(output_image.numpy())\n    return output_image\nprint(\"done\")","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:20:13.754021Z","iopub.execute_input":"2024-04-18T18:20:13.754413Z","iopub.status.idle":"2024-04-18T18:20:13.762841Z","shell.execute_reply.started":"2024-04-18T18:20:13.754384Z","shell.execute_reply":"2024-04-18T18:20:13.761735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n# create inference function\ndef inference(o_image):\n# preprocess for the model\n    image = keras.preprocessing.image.ing_to_array(o_image)\n    image = image.astype(\"float32\") / 255.0\n    image = np. expand_dims (image, axis=0) # create batch of 1 image\n    print (image. shape) # should be (1, H, w, C)\n    output_image = model (image) # run the image through model\n    output_image = post_process (image, output_image) # will implement this later\n    output_image = tf.cast((output_image[0, :, :, :] * 255), dtype=np.uint8) # processing for PIL\n    output_image = Image. fromarray (output_image.numpy())\n    return ouput_image\n'''","metadata":{"execution":{"iopub.status.busy":"2024-03-24T05:42:19.083903Z","iopub.execute_input":"2024-03-24T05:42:19.084932Z","iopub.status.idle":"2024-03-24T05:42:19.092221Z","shell.execute_reply.started":"2024-03-24T05:42:19.084865Z","shell.execute_reply":"2024-03-24T05:42:19.090943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:26:47.735343Z","iopub.execute_input":"2024-04-18T18:26:47.735750Z","iopub.status.idle":"2024-04-18T18:26:47.747525Z","shell.execute_reply.started":"2024-04-18T18:26:47.735720Z","shell.execute_reply":"2024-04-18T18:26:47.746290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from PIL import Image \nimport numpy as np\n\ndef plot_results(images, titles, figure_size=(12, 12)):\n    fig = plt.figure(figsize=figure_size)\n    for i in range(len(images)):\n        fig.add_subplot(1, len(images), i + 1).set_title(titles[i])\n        _ = plt.imshow(images[i])\n        plt.axis(\"off\")\n    plt.show()\n\ntest_low_light_images = sorted(glob(\"/kaggle/input/darkface/image/*.png\"))[100:150]\nfor val_image_file in test_low_light_images:\n    original_image = Image.open(val_image_file)\n    enhanced_image = infer(original_image)\n    plot_results(\n        [original_image, enhanced_image],\n        [\"Original\", \"Enhanced\"],\n        (20, 12)\n    )\n\n    # Convert PIL Images to numpy arrays\n    original_array = np.array(original_image)\n    enhanced_array = np.array(enhanced_image)\n\n    # Perform subtraction\n    subtracted_image_array = original_array - enhanced_array\n\n    # Convert the result back to PIL Image\n    subtracted_image = Image.fromarray(subtracted_image_array)\n\n    # Display or save the subtracted image as needed\n    print(subtracted_image_array)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:35:48.406429Z","iopub.execute_input":"2024-04-18T18:35:48.407661Z","iopub.status.idle":"2024-04-18T18:38:37.554450Z","shell.execute_reply.started":"2024-04-18T18:35:48.407596Z","shell.execute_reply":"2024-04-18T18:38:37.553131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"HERE WE TRY TO DO THE IMAGE LIGHTENING USING MIRNET","metadata":{}},{"cell_type":"code","source":"def selective_kernel_feature_fusion(\n    multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3\n):\n    channels = list(multi_scale_feature_1.shape)[-1]\n    combined_feature = layers.Add()(\n        [multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3]\n    )\n    gap = layers.GlobalAveragePooling2D()(combined_feature)\n    channel_wise_statistics = layers.Reshape((1, 1, channels))(gap)\n    compact_feature_representation = layers.Conv2D(\n        filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"\n    )(channel_wise_statistics)\n    feature_descriptor_1 = layers.Conv2D(\n        channels, kernel_size=(1, 1), activation=\"softmax\"\n    )(compact_feature_representation)\n    feature_descriptor_2 = layers.Conv2D(\n        channels, kernel_size=(1, 1), activation=\"softmax\"\n    )(compact_feature_representation)\n    feature_descriptor_3 = layers.Conv2D(\n        channels, kernel_size=(1, 1), activation=\"softmax\"\n    )(compact_feature_representation)\n    feature_1 = multi_scale_feature_1 * feature_descriptor_1\n    feature_2 = multi_scale_feature_2 * feature_descriptor_2\n    feature_3 = multi_scale_feature_3 * feature_descriptor_3\n    aggregated_feature = layers.Add()([feature_1, feature_2, feature_3])\n    return aggregated_feature\nprint(\"done\")","metadata":{"execution":{"iopub.status.busy":"2024-04-18T19:27:06.020229Z","iopub.execute_input":"2024-04-18T19:27:06.020694Z","iopub.status.idle":"2024-04-18T19:27:06.032847Z","shell.execute_reply.started":"2024-04-18T19:27:06.020661Z","shell.execute_reply":"2024-04-18T19:27:06.031703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ChannelPooling(layers.Layer):\n    def __init__(self, axis=-1, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.axis = axis\n        self.concat = layers.Concatenate(axis=self.axis)\n\n    def call(self, inputs):\n        average_pooling = tf.expand_dims(tf.reduce_mean(inputs, axis=-1), axis=-1)\n        max_pooling = tf.expand_dims(tf.reduce_max(inputs, axis=-1), axis=-1)\n        return self.concat([average_pooling, max_pooling])\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\"axis\": self.axis})\n\n\ndef spatial_attention_block(input_tensor):\n    compressed_feature_map = ChannelPooling(axis=-1)(input_tensor)\n    feature_map = layers.Conv2D(1, kernel_size=(1, 1))(compressed_feature_map)\n    feature_map = keras.activations.sigmoid(feature_map)\n    return input_tensor * feature_map\n\n\ndef channel_attention_block(input_tensor):\n    channels = list(input_tensor.shape)[-1]\n    average_pooling = layers.GlobalAveragePooling2D()(input_tensor)\n    feature_descriptor = layers.Reshape((1, 1, channels))(average_pooling)\n    feature_activations = layers.Conv2D(\n        filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"\n    )(feature_descriptor)\n    feature_activations = layers.Conv2D(\n        filters=channels, kernel_size=(1, 1), activation=\"sigmoid\"\n    )(feature_activations)\n    return input_tensor * feature_activations\n\n\ndef dual_attention_unit_block(input_tensor):\n    channels = list(input_tensor.shape)[-1]\n    feature_map = layers.Conv2D(\n        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n    )(input_tensor)\n    feature_map = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(\n        feature_map\n    )\n    channel_attention = channel_attention_block(feature_map)\n    spatial_attention = spatial_attention_block(feature_map)\n    concatenation = layers.Concatenate(axis=-1)([channel_attention, spatial_attention])\n    concatenation = layers.Conv2D(channels, kernel_size=(1, 1))(concatenation)\n    return layers.Add()([input_tensor, concatenation])\nprint(\"done\")","metadata":{"execution":{"iopub.status.busy":"2024-04-18T19:27:16.233485Z","iopub.execute_input":"2024-04-18T19:27:16.233910Z","iopub.status.idle":"2024-04-18T19:27:16.253862Z","shell.execute_reply.started":"2024-04-18T19:27:16.233878Z","shell.execute_reply":"2024-04-18T19:27:16.252179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Recursive Residual Modules\n\n\ndef down_sampling_module(input_tensor):\n    channels = list(input_tensor.shape)[-1]\n    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n        input_tensor\n    )\n    main_branch = layers.Conv2D(\n        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n    )(main_branch)\n    main_branch = layers.MaxPooling2D()(main_branch)\n    main_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(main_branch)\n    skip_branch = layers.MaxPooling2D()(input_tensor)\n    skip_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(skip_branch)\n    return layers.Add()([skip_branch, main_branch])\n\n\ndef up_sampling_module(input_tensor):\n    channels = list(input_tensor.shape)[-1]\n    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n        input_tensor\n    )\n    main_branch = layers.Conv2D(\n        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n    )(main_branch)\n    main_branch = layers.UpSampling2D()(main_branch)\n    main_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(main_branch)\n    skip_branch = layers.UpSampling2D()(input_tensor)\n    skip_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(skip_branch)\n    return layers.Add()([skip_branch, main_branch])\n\n\n# MRB Block\ndef multi_scale_residual_block(input_tensor, channels):\n    # features\n    level1 = input_tensor\n    level2 = down_sampling_module(input_tensor)\n    level3 = down_sampling_module(level2)\n    # DAU\n    level1_dau = dual_attention_unit_block(level1)\n    level2_dau = dual_attention_unit_block(level2)\n    level3_dau = dual_attention_unit_block(level3)\n    # SKFF\n    level1_skff = selective_kernel_feature_fusion(\n        level1_dau,\n        up_sampling_module(level2_dau),\n        up_sampling_module(up_sampling_module(level3_dau)),\n    )\n    level2_skff = selective_kernel_feature_fusion(\n        down_sampling_module(level1_dau),\n        level2_dau,\n        up_sampling_module(level3_dau),\n    )\n    level3_skff = selective_kernel_feature_fusion(\n        down_sampling_module(down_sampling_module(level1_dau)),\n        down_sampling_module(level2_dau),\n        level3_dau,\n    )\n    # DAU 2\n    level1_dau_2 = dual_attention_unit_block(level1_skff)\n    level2_dau_2 = up_sampling_module((dual_attention_unit_block(level2_skff)))\n    level3_dau_2 = up_sampling_module(\n        up_sampling_module(dual_attention_unit_block(level3_skff))\n    )\n    # SKFF 2\n    skff_ = selective_kernel_feature_fusion(level1_dau_2, level2_dau_2, level3_dau_2)\n    conv = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(skff_)\n    return layers.Add()([input_tensor, conv])\nprint(\"done\")","metadata":{"execution":{"iopub.status.busy":"2024-04-18T19:27:20.401910Z","iopub.execute_input":"2024-04-18T19:27:20.402309Z","iopub.status.idle":"2024-04-18T19:27:20.421885Z","shell.execute_reply.started":"2024-04-18T19:27:20.402281Z","shell.execute_reply":"2024-04-18T19:27:20.420458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def recursive_residual_group(input_tensor, num_mrb, channels):\n    conv1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(input_tensor)\n    for _ in range(num_mrb):\n        conv1 = multi_scale_residual_block(conv1, channels)\n    conv2 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(conv1)\n    return layers.Add()([conv2, input_tensor])\n\n\ndef mirnet_model(num_rrg, num_mrb, channels):\n    input_tensor = keras.Input(shape=[None, None, 3])\n    x1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(input_tensor)\n    for _ in range(num_rrg):\n        x1 = recursive_residual_group(x1, num_mrb, channels)\n    conv = layers.Conv2D(3, kernel_size=(3, 3), padding=\"same\")(x1)\n    output_tensor = layers.Add()([input_tensor, conv])\n    return keras.Model(input_tensor, output_tensor)\n\n\nmodel = mirnet_model(num_rrg=3, num_mrb=2, channels=64)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T19:27:28.192795Z","iopub.execute_input":"2024-04-18T19:27:28.193177Z","iopub.status.idle":"2024-04-18T19:27:35.374501Z","shell.execute_reply.started":"2024-04-18T19:27:28.193148Z","shell.execute_reply":"2024-04-18T19:27:35.373286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport cv2\n\ndef charbonnier_loss(y_true, y_pred):\n    loss = tf.sqrt(tf.square(y_true - y_pred) + tf.square(1e-3))\n    print(\"charbonnier_loss:\", loss)\n    return loss\n\nx = cv2.imread(\"/kaggle/input/darkface/image/1.png\")\ny = cv2.imread(\"/kaggle/input/enhanced-input/1.png\")\n\n# Convert NumPy arrays to TensorFlow tensors with dtype uint8\nx_tensor = tf.convert_to_tensor(x, dtype=tf.uint8)\ny_tensor = tf.convert_to_tensor(y, dtype=tf.uint8)\n\nl = charbonnier_loss(x_tensor, y_tensor)\nprint(l)\n\n'''\ndef peak_signal_noise_ratio(y_true, y_pred):\n    psnr = tf.image.psnr(y_pred, y_true, max_val=255.0)\n    print(\"peak_signal_noise_ratio:\", psnr)\n    if(psnr==None):\n        print(1)\n    return psnr\n\n\noptimizer = keras.optimizers.Adam(learning_rate=1e-4)\nmodel.compile(\n    optimizer=optimizer,\n    loss=keras.losses.SparseCategoricalCrossentropy(),\n    metrics=[peak_signal_noise_ratio],\n)\n\nhistory = model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=1,\n    callbacks=[\n        keras.callbacks.ReduceLROnPlateau(\n            monitor=\"val_peak_signal_noise_ratio\",\n            factor=0.5,\n            patience=5,\n            verbose=1,\n            min_delta=1e-7,\n            mode=\"max\",\n        )\n    ],\n)\n\n\ndef plot_history(value, name):\n    plt.plot(history.history[value], label=f\"train_{name.lower()}\")\n    plt.plot(history.history[f\"val_{value}\"], label=f\"val_{name.lower()}\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(name)\n    plt.title(f\"Train and Validation {name} Over Epochs\", fontsize=14)\n    plt.legend()\n    plt.grid()\n    plt.show()\n\n\nplot_history(\"loss\", \"Loss\")\nplot_history(\"peak_signal_noise_ratio\", \"PSNR\")\n'''","metadata":{"execution":{"iopub.status.busy":"2024-04-18T19:39:38.376743Z","iopub.execute_input":"2024-04-18T19:39:38.377140Z","iopub.status.idle":"2024-04-18T19:39:38.574260Z","shell.execute_reply.started":"2024-04-18T19:39:38.377112Z","shell.execute_reply":"2024-04-18T19:39:38.572694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"IMAGE LIGHTENING USING THE RETINEXNET","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:23:34.340069Z","iopub.execute_input":"2024-05-04T19:23:34.340545Z","iopub.status.idle":"2024-05-04T19:23:34.345500Z","shell.execute_reply.started":"2024-05-04T19:23:34.340510Z","shell.execute_reply":"2024-05-04T19:23:34.344403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def RelightNet(input_L, input_R):\n    input_im = tf.concat([input_R, input_L])\n    with tf.variable_scope('RelightNet', reuse=tf.AUTO_REUSE):\n        conv0 = tf.layers.conv2d(input_im, 64, 3, padding='same', activation=None)\n        conv1 = tf.layers.conv2d(conv0, 64, 3, strides=2, padding='same', activation=tf.nn.relu)\n        conv2 = tf.layers.conv2d(conv1, 64,3, strides=2, padding='same', activation=tf.nn.relu)\n        conv3 = tf.layers.conv2d(conv2, 64, 3, strides=2, padding='same', activation=tf.nn.relu)\n        \n        up1 = tf.image.resize_nearest_neighbor(conv3, (tf.shape(conv2)[1], tf.shape(conv2)[2]))\n        deconv1 = tf.layers.conv2d(up1, 64, 3, padding='same', activation=tf.nn.relu) + conv2\n        up2 = tf.image.resize_nearest_neighbor(deconv1, (tf.shape(conv1)[1], tf.shape(conv1)[2]))\n        deconv2= tf.layers.conv2d(up2, 64, 3, padding='same', activation=tf.nn.relu) + conv1\n        up3 = tf.image.resize_nearest_neighbor(deconv2, (tf.shape(conv0)[1], tf.shape(conv0)[2]))\n        deconv3 = tf.layers.conv2d(up3, 64,3, padding='same', activation=tf.nn.relu) + conv0\n        \n        deconv1_resize = tf.image.resize_nearest_neighbor(deconv1, (tf.shape(deconv3)[1], tf.shape(deconv3)[2]))\n        deconv2_resize = tf.image.resize_nearest_neighbor(deconv2, (tf.shape(deconv3)[1], tf.shape(deconv3)[2]))\n        feature_gather = concat([deconv1_resize, deconv2_resize, deconv3])\n        feature_fusion = tf.layers.conv2d(feature_gather, 64, 1, padding='same', activation=None)\n        output = tf.layers.conv2d(feature_fusion, 1, 3, padding='same', activation=None)\n    return output","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:59:55.907378Z","iopub.execute_input":"2024-05-04T19:59:55.907732Z","iopub.status.idle":"2024-05-04T19:59:55.919236Z","shell.execute_reply.started":"2024-05-04T19:59:55.907706Z","shell.execute_reply":"2024-05-04T19:59:55.917934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport time\nfrom tqdm import tqdm\nimport random\n\nclass lowlight_enhance_2:\n\n    def __init__(self, train_high_data,  Reflectance, Illuminance, Reflectance_test, Illuminance_test, batch_size, patch_size, epoch, learning_rate, train_phase):\n\n        self.train_high_data = train_high_data\n\n        self.R_low = Reflectance\n        self.I_low = Illuminance\n\n        self.R_low_test = Reflectance_test\n        self.I_low_test = Illuminance_test\n\n        self.batch_size = batch_size\n        self.patch_size = patch_size\n        self.epoch = epoch\n        self.learning_rate = learning_rate\n        self.train_phase = train_phase\n\n        self.train_high_data_ph = tf.keras.Input(shape=(None, None, 3), name='train_high_data_ph')\n        self.R_low_ph = tf.keras.Input(shape=(None, None, 3), name='R_low_ph')\n        self.I_low_ph = tf.keras.Input(shape=(None, None, 3), name='I_low_ph')\n\n        self.lr_ph = tf.keras.Input(shape=(), name='lr_ph')\n\n        I_delta = RelightNet(self.I_low_ph, self.R_low_ph)\n\n        I_delta_3 = tf.concat([I_delta, I_delta, I_delta], axis=-1)\n\n        self.output_I_delta = I_delta_3\n        self.output_S = self.R_low_ph * I_delta_3\n\n        self.relight_loss = tf.reduce_mean(tf.abs(self.R_low_ph * I_delta_3 - self.train_high_data_ph))\n\n        self.Ismooth_loss_delta = self.smooth(I_delta, self.R_low_ph)\n \n        self.loss_Relight = self.relight_loss + 3 * self.Ismooth_loss_delta\n\n        self.lr_ph = tf.keras.Input(shape=(), name='learning_rate')\n        optimizer = tf.keras.optimizers.Adam(self.lr_ph)\n \n        self.var_Relight = [var for var in tf.trainable_variables() if 'RelightNet' in var.name]\n\n        self.train_op_Relight = optimizer.minimize(self.loss_Relight, var_list = self.var_Relight)\n\n        self.saver_Relight = tf.train.Saver(var_list = self.var_Relight)\n\n        print(\"[*] Initialize model successfully...\")\n\n    def gradient(self, input_tensor, direction):\n        self.smooth_kernel_x = tf.reshape(tf.constant([[0, 0], [-1, 1]], tf.float32), [2, 2, 1, 1])\n        self.smooth_kernel_y = tf.transpose(self.smooth_kernel_x, [1, 0, 2, 3])\n\n        if direction == \"x\":\n            kernel = self.smooth_kernel_x\n        elif direction == \"y\":\n            kernel = self.smooth_kernel_y\n        return tf.abs(tf.nn.conv2d(input_tensor, kernel, strides=[1, 1, 1, 1], padding='SAME'))\n\n    def ave_gradient(self, input_tensor, direction):\n        return tf.nn.avg_pool2d(self.gradient(input_tensor, direction), ksize=3, strides=1, padding='SAME')\n\n    def smooth(self, input_I, input_R):\n        input_R = tf.image.rgb_to_grayscale(input_R)\n        return tf.reduce_mean(self.gradient(input_I, \"x\") * tf.exp(-10 * self.ave_gradient(input_R, \"x\")) + self.gradient(input_I, \"y\") * tf.exp(-10 * self.ave_gradient(input_R, \"y\")))\n    \n    def evaluate_train(self):\n\n        print(\"Evaluating for train data\")\n        R_low_hat = []\n        I_low_hat = []\n\n        for idx in tqdm(range(len(self.R_low))):\n        #for idx in tqdm(range(start_id, end_id)):\n            input_R_low = np.expand_dims(self.R_low[idx], axis=0)\n            input_I_low = np.expand_dims(self.I_low[idx], axis=0)\n\n            if self.train_phase == \"Relight\":\n                result_1, result_2 = self.sess.run([self.output_S, self.output_I_delta], feed_dict={self.R_low_ph: input_R_low, self.I_low_ph: input_I_low,})\n\n            R_low_hat.append(result_1)\n            I_low_hat.append(result_2)\n\n        return R_low_hat, I_low_hat\n    \n    def train(self):\n \n        numBatch = 30\n    \n        train_op = self.train_op_Relight\n        train_loss = self.loss_Relight\n        saver = self.saver_Relight\n\n        iter_num = 0\n        start_epoch = 0\n        start_step = 0\n        lr1 = self.learning_rate\n\n        start_time = time.time()\n        image_id = 0\n        \n        for epoch in range(start_epoch, self.epoch):\n\n            for batch_id in range(start_step, numBatch):\n\n        # generate data for a batch\n                batch_R_low = np.zeros((self.batch_size, 128, 128, 3), dtype=\"float32\")\n                batch_I_low = np.zeros((self.batch_size, 128, 128, 3), dtype=\"float32\")\n                batch_input_high = np.zeros((self.batch_size, 128, 128, 3), dtype=\"float32\")\n\n                for patch_id in range(self.batch_size):\n                    batch_R_low[patch_id, :, :, :] = self.R_low[image_id, :, :, :]\n                    batch_I_low[patch_id, :, :, :] = self.I_low[image_id, :, :, :]\n                    batch_input_high[patch_id, :, :, :] = self.train_high_data[image_id][:,:,:]\n\n                    image_id = (image_id + 1) % len(self.R_low)\n                    if image_id == 0:\n                        tmp = list(zip(self.R_low, self.I_low))\n                        random.shuffle(list(tmp))\n                        R_low, I_low  = zip(*tmp)\n                # train\n                _, loss = self.sess.run([train_op, train_loss], feed_dict={self.R_low_ph: batch_R_low, \\\n                                                                    self.I_low_ph: batch_I_low, \\\n                                                                    self.lr_ph: lr1[epoch], \\\n                                                                    self.train_high_data_ph: batch_input_high})\n        \n                print(\"%s Epoch: [%2d] [%4d/%4d] time: %4.4f, loss: %.6f\" \\\n                      % (self.train_phase, epoch + 1, batch_id + 1, numBatch, time.time() - start_time, loss))\n                iter_num += 1 \n\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:00:01.517420Z","iopub.execute_input":"2024-05-04T20:00:01.517782Z","iopub.status.idle":"2024-05-04T20:00:01.544429Z","shell.execute_reply.started":"2024-05-04T20:00:01.517754Z","shell.execute_reply":"2024-05-04T20:00:01.543124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2 = lowlight_enhance_2(train_dataset, 0.12, 0.5, 0.9, 0.67, 10,5 , 10, 0.01, \"RelightNet\")\nOutput_S, Output_I_delta = model2.evaluate_train()","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:00:09.047499Z","iopub.execute_input":"2024-05-04T20:00:09.047868Z","iopub.status.idle":"2024-05-04T20:00:09.180874Z","shell.execute_reply.started":"2024-05-04T20:00:09.047834Z","shell.execute_reply":"2024-05-04T20:00:09.179378Z"},"trusted":true},"execution_count":null,"outputs":[]}]}